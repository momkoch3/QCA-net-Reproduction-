{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7893370,"sourceType":"datasetVersion","datasetId":4634592},{"sourceId":7893505,"sourceType":"datasetVersion","datasetId":4634700}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch as torch\nimport torch.nn as nn\nfrom torchvision import transforms, utils\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport matplotlib.pyplot as plt\nfrom torchinfo import summary\nfrom skimage import io\nimport os\nimport pandas as pd\nfrom skimage import transform as tr\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport tqdm\nfrom datetime import datetime\nimport pytz","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:19.298Z","iopub.execute_input":"2024-03-26T17:00:19.299653Z","iopub.status.idle":"2024-03-26T17:00:19.309446Z","shell.execute_reply.started":"2024-03-26T17:00:19.299593Z","shell.execute_reply":"2024-03-26T17:00:19.308228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating arrays that store all the training and test data paths\n\n#NOTE THAT THE DATA SHOULD BE UPLOADED ON KAGGLE AND ADDED AS INPUT (See FILE-> ADD INPUT)\n#THESE ROOT PATHS SHOULD BE CHANGED ACCORDING TO YOUR KAGGLE SETUP!\n\n#THESE ROOT PATHS SHOULD BE CHANGED ACCORDING TO YOUR SETUP!\nroot = \"/kaggle/input/embryodata/\"\ntrain_data_path = root + \"Images/Images/train/Images/\"\ntest_data_path = root + \"Images/Images/test/Images/\"\nground_truth_data_path = root + \"GroundTruth/GroundTruth/train/GroundTruth_NDN/\"\nground_truth_test = root + \"GroundTruth/GroundTruth/test/GroundTruth_QCANet/\"\n\ntrain_data_paths = np.array([(train_data_path+filename) for filename in os.listdir(train_data_path) if 'Emb11' not in filename])\nval_data_paths = np.array([(train_data_path+filename) for filename in os.listdir(train_data_path) if 'Emb11' in filename])\ntest_data_paths = np.array([test_data_path+i for i in os.listdir(test_data_path)])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:19.343628Z","iopub.execute_input":"2024-03-26T17:00:19.344005Z","iopub.status.idle":"2024-03-26T17:00:19.369585Z","shell.execute_reply.started":"2024-03-26T17:00:19.343977Z","shell.execute_reply":"2024-03-26T17:00:19.36865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Custom Torch dataset class to create a dataset and pass it to Torch dataloaders\nclass EmbryoImageDataset(Dataset):\n    def __init__(self, \n                 image_paths, \n                 ground_truth_dir, \n                 transform=None, \n                 flip_augmentation = [False, None],\n                test_data = False):\n        self.ground_truth_dir = ground_truth_dir\n        self.image_paths = image_paths\n        self.transform = transform\n        self.flip_augmentation = flip_augmentation\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        \n        image = io.imread(self.image_paths[index])\n        filename = self.image_paths[index].split('/')[-1]\n        ground_truth = io.imread(self.ground_truth_dir + filename)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            ground_truth = self.transform(ground_truth)\n            \n        if self.flip_augmentation[0]:\n            #print(\"FLIPPING\", self.flip_augmentation[0], self.flip_augmentation[1] )\n            image = np.flip(image, axis = self.flip_augmentation[1])\n            ground_truth = np.flip(ground_truth, axis = self.flip_augmentation[1])\n            \n        if test_data: \n            #Ground truth of test data was instance segmentation ground truth, therefore all values bigger than 0 should be set to 1\n            ground_truth = ((ground_truth>0)*1).astype(ground_truth.dtype) \n    \n        return torch.from_numpy(image.copy().astype('float32')), torch.from_numpy(ground_truth.copy().astype('float32'))\n    \n\nclass NormalizeImage(object):\n    \n    def __init__(self):\n        pass\n    \n    def __call__(self, image):\n        \n        img = (image-np.min(image))/(np.max(image)-np.min(image))\n        \n        return img\n    \n#Class that can perform a transform that interpolates the images along z, according to the resolution. \n# This class also makes sure that the object is of the desired size. In this case 128x128x128\nclass Resize_and_pad(object):\n    \n    def __init__(self, resolution = [0.8,0.8,1.75], desired_size = 128):\n        \n        self.resolution = resolution\n        self.desired_size = desired_size\n        \n    def __call__(self, image):\n        \n        \n        shape = np.shape(image)\n        resize_factor = np.array(self.resolution)/0.8\n\n        resized_image = tr.resize(image, \n                                  (shape[0]*resize_factor[2], \n                                   shape[1]*resize_factor[1], \n                                   shape[2]*resize_factor[0]),\n                                     order=1)\n        \n        delta_shape = [128-value for value in resized_image.shape]\n        max_pad_width = np.max(delta_shape)\n\n        padded_image = np.pad(resized_image,[[0,max_pad_width],[0,max_pad_width],[0,max_pad_width]], mode = 'reflect')\n        final_image = padded_image[:self.desired_size,:self.desired_size,:self.desired_size]\n        \n        return final_image\n        ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:19.384099Z","iopub.execute_input":"2024-03-26T17:00:19.384519Z","iopub.status.idle":"2024-03-26T17:00:19.40152Z","shell.execute_reply.started":"2024-03-26T17:00:19.384479Z","shell.execute_reply":"2024-03-26T17:00:19.400248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Here the train and test dataset are created\n\n#Composing transforms (normalization, resizing and padding)\ncomposed = transforms.Compose([NormalizeImage(), Resize_and_pad(resolution= [0.8,0.8,1.75])])\n#For the test set there was a different resolution so different transformation is needed\ncomposed_test = composed = transforms.Compose([NormalizeImage(), Resize_and_pad(resolution= [0.8,0.8,2.0])])\n\n#creating standard train dataset\ntrain_data = EmbryoImageDataset(image_paths = train_data_paths,\n                               ground_truth_dir = ground_truth_data_path,\n                               transform = composed)\n\n#creating the validation dataset for evaluating the model during training\nval_data = EmbryoImageDataset(image_paths = val_data_paths,\n                               ground_truth_dir = ground_truth_data_path,\n                               transform = composed)\n\n#Creating the test dataset for when the model is fully trained\ntest_data = EmbryoImageDataset(image_paths = test_data_paths,\n                              ground_truth_dir= ground_truth_test,\n                              test_data= True, transform = composed_test)\n\n#Here data augmentation is performed by creating additional datasets that flip over the x-,y- and x&y-axis.\n# This gives x4 the original datasize\ntrain_data_flipx = EmbryoImageDataset(image_paths = train_data_paths,\n                               ground_truth_dir = ground_truth_data_path,\n                               transform = composed,\n                                flip_augmentation= [True, 1])\ntrain_data_flipy = EmbryoImageDataset(image_paths = train_data_paths,\n                               ground_truth_dir = ground_truth_data_path,\n                               transform = composed,\n                                flip_augmentation= [True, 2])\ntrain_data_flipxy = EmbryoImageDataset(image_paths = train_data_paths,\n                               ground_truth_dir = ground_truth_data_path,\n                               transform = composed,\n                                flip_augmentation= [True, (0,1)])\n\n#Here all the train datasets are combined to form the augmented dataset with x4 data size\naugmented_train_data = torch.utils.data.ConcatDataset([train_data,train_data_flipx,train_data_flipy,train_data_flipxy])\n\nbatch_size = 1\n#Creating the dataloaders based on the created datasets\ntrain_loader = DataLoader(augmented_train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size,shuffle=True)\n\n#Printing the amount of training/test images in both datasets\nprint(len(augmented_train_data))\nprint(len(val_data))\nprint(len(test_data))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:19.403552Z","iopub.execute_input":"2024-03-26T17:00:19.404394Z","iopub.status.idle":"2024-03-26T17:00:19.420485Z","shell.execute_reply.started":"2024-03-26T17:00:19.404361Z","shell.execute_reply":"2024-03-26T17:00:19.419504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#Creating small training and test set for testing purposes\ntrain_data_small = Subset(train_data, [i for i in range(0,10)])\nval_data_small = Subset(val_data, [i for i in range(0,2)])\n\ntrain_loader_small = DataLoader(train_data_small, batch_size=1, shuffle=True)\nval_loader_small = DataLoader(val_data_small, batch_size=1,shuffle=True)\n\nprint(len(train_data_small), len(val_data_small))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T17:00:19.42339Z","iopub.execute_input":"2024-03-26T17:00:19.424013Z","iopub.status.idle":"2024-03-26T17:00:19.437743Z","shell.execute_reply.started":"2024-03-26T17:00:19.423981Z","shell.execute_reply":"2024-03-26T17:00:19.436549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing a random training sample from the training dataset\n\nN = 64\nfig, ax = plt.subplots(1, 2)\n\nindex = int(np.random.rand()*len(augmented_train_data))\n\nimage, truth = augmented_train_data[index]\n\nax[0].imshow(image[N])\nax[0].set_title(\"Image\")\nax[1].imshow(truth[N])\nax[1].set_title(\"Ground Truth\")\nplt.show()\n\nprint(image.shape, truth.shape)\nprint(image.dtype, truth.dtype)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:19.449768Z","iopub.execute_input":"2024-03-26T17:00:19.450909Z","iopub.status.idle":"2024-03-26T17:00:20.227613Z","shell.execute_reply.started":"2024-03-26T17:00:19.450873Z","shell.execute_reply":"2024-03-26T17:00:20.226526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing a random test sample of the validation dataset\n\nN = 64\nfig, ax = plt.subplots(1, 2)\n\nindex = int(np.random.rand()*len(val_data))\n\nimage, truth = val_data[index]\n\nax[0].imshow(image[N])\nax[0].set_title(\"Image\")\nax[1].imshow(truth[N])\nax[1].set_title(\"Ground Truth\")\nplt.show()\n\nprint(image.shape, truth.shape)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:20.229876Z","iopub.execute_input":"2024-03-26T17:00:20.230481Z","iopub.status.idle":"2024-03-26T17:00:21.010227Z","shell.execute_reply.started":"2024-03-26T17:00:20.230419Z","shell.execute_reply":"2024-03-26T17:00:21.009052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NDN_Net(nn.Module):\n    def __init__(self, in_channels = 1, out_channels = 1):\n        super(NDN_Net, self).__init__()\n        \n        #Activation functions and maxpooling\n        self.relu = nn.ReLU()\n        self.sig = nn.Sigmoid() \n        self.maxpool = nn.MaxPool3d(kernel_size=(2,2,2), stride=2)\n        \n        #Encoder Block 1\n        self.conv1 = nn.Conv3d(in_channels = in_channels, out_channels = 12, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn1 = nn.BatchNorm3d(12)\n        self.conv2 = nn.Conv3d(in_channels = 12, out_channels = 24, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn2 = nn.BatchNorm3d(24)\n        \n        #Encoder Block 2\n        self.conv3 = nn.Conv3d(in_channels = 24, out_channels = 24, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn3 = nn.BatchNorm3d(24)\n        self.conv4 = nn.Conv3d(in_channels = 24, out_channels = 48, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn4 = nn.BatchNorm3d(48)\n        \n        #Encoder Block 3\n        self.conv5 = nn.Conv3d(in_channels = 48, out_channels = 48, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn5 = nn.BatchNorm3d(48)\n        self.conv6 = nn.Conv3d(in_channels = 48, out_channels = 96, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn6 = nn.BatchNorm3d(96)\n        \n        #Encoder Block 4\n        self.conv7 = nn.Conv3d(in_channels = 96, out_channels = 96, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn7 = nn.BatchNorm3d(96)\n        self.conv8 = nn.Conv3d(in_channels = 96, out_channels = 192, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn8 = nn.BatchNorm3d(192)\n        \n        #Bottleneck block\n        self.conv9 = nn.Conv3d(in_channels = 192, out_channels = 192, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn9 = nn.BatchNorm3d(192)\n        self.conv10 = nn.Conv3d(in_channels = 192, out_channels = 384, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn10 = nn.BatchNorm3d(384)\n        \n        #Decoder Block 1\n        self.upconv1 = nn.ConvTranspose3d(in_channels = 384, out_channels = 384, kernel_size = (2,2,2), padding = 0, stride = 2)\n        self.conv11 = nn.Conv3d(in_channels = 384+192, out_channels = 192, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn11 = nn.BatchNorm3d(192)\n        self.conv12 = nn.Conv3d(in_channels = 192, out_channels = 192, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn12 = nn.BatchNorm3d(192)\n        \n        #Decoder Block 2\n        self.upconv2 = nn.ConvTranspose3d(in_channels = 192, out_channels = 192, kernel_size = (2,2,2), padding = 0, stride = 2)\n        self.conv13 = nn.Conv3d(in_channels = 192+96, out_channels = 96, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn13 = nn.BatchNorm3d(96)\n        self.conv14 = nn.Conv3d(in_channels = 96, out_channels = 96, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn14 = nn.BatchNorm3d(96)\n        \n        #Decoder Block 3 \n        self.upconv3 = nn.ConvTranspose3d(in_channels = 96, out_channels = 96, kernel_size = (2,2,2), stride = 2)\n        self.conv15 = nn.Conv3d(in_channels = 96+48, out_channels = 48, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn15 = nn.BatchNorm3d(48)\n        self.conv16 = nn.Conv3d(in_channels = 48, out_channels = 48, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn16 = nn.BatchNorm3d(48)\n        \n        #Decoder Block 4\n        self.upconv4 = nn.ConvTranspose3d(in_channels = 48, out_channels = 48, kernel_size = (2,2,2), stride = 2)\n        self.conv17 = nn.Conv3d(in_channels = 48+24, out_channels = 24, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn17 = nn.BatchNorm3d(24)\n        self.conv18 = nn.Conv3d(in_channels = 24, out_channels = 24, kernel_size = (5,5,5), padding = 2, stride = 1)\n        self.bn18 = nn.BatchNorm3d(24)\n        \n        #Final convolution\n        self.conv19 = nn.Conv3d(in_channels = 24, out_channels = out_channels, kernel_size = (1,1,1), padding = 0, stride = 1)\n        \n    def forward(self, input):\n\n        #Encoder Block 1\n        out = self.relu(self.bn1(self.conv1(input)))\n        out = self.relu(self.bn2(self.conv2(out)))\n        skip_connection1 = out\n        out = self.maxpool(out)\n\n        #Encoder Block 2\n        out = self.relu(self.bn3(self.conv3(out)))\n        out = self.relu(self.bn4(self.conv4(out)))\n        skip_connection2 = out\n        out = self.maxpool(out)\n\n        #Encoder Block 3\n        out = self.relu(self.bn5(self.conv5(out)))\n        out = self.relu(self.bn6(self.conv6(out)))\n        skip_connection3 = out\n        out = self.maxpool(out)\n        \n        #Encoder Block 4\n        out = self.relu(self.bn7(self.conv7(out)))\n        out = self.relu(self.bn8(self.conv8(out)))\n        skip_connection4 = out\n        out = self.maxpool(out)\n\n        #Bottleneck layer\n        out = self.relu(self.bn9(self.conv9(out)))\n        out = self.relu(self.bn10(self.conv10(out)))\n\n        #Decoder Block 1\n        out = self.upconv1(out)\n        out = torch.cat(tensors = (out, skip_connection4), dim = 1)\n        out = self.relu(self.bn11(self.conv11(out)))\n        out = self.relu(self.bn12(self.conv12(out)))\n\n        #Decoder Block 2\n        out = self.upconv2(out)\n        out = torch.cat(tensors = (out, skip_connection3), dim = 1)\n        out = self.relu(self.bn13(self.conv13(out)))\n        out = self.relu(self.bn14(self.conv14(out)))\n\n        #Decoder Block 3\n        out = self.upconv3(out)\n        out = torch.cat(tensors = (out, skip_connection2), dim = 1)\n        out = self.relu(self.bn15(self.conv15(out)))\n        out = self.relu(self.bn16(self.conv16(out)))\n\n        #Decoder Block 4\n        out = self.upconv4(out)\n        out = torch.cat(tensors = (out, skip_connection1), dim = 1)\n        out = self.relu(self.bn17(self.conv17(out)))\n        out = self.relu(self.bn18(self.conv18(out)))\n        \n        #Final convolution + sigmoid activation\n        out = self.conv19(out)\n        out = self.sig(out) #?\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-26T17:00:21.01189Z","iopub.execute_input":"2024-03-26T17:00:21.012276Z","iopub.status.idle":"2024-03-26T17:00:21.042662Z","shell.execute_reply.started":"2024-03-26T17:00:21.012245Z","shell.execute_reply":"2024-03-26T17:00:21.041621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Specifying the device, testing whether there is a GPU available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Current device: {device}')\n\n#Creating an instance of the network and moving it to the created device\nnet = NDN_Net(in_channels= 1, out_channels = 1).to(device)\n\n#Set optimizer to Adam\nlr = 0.0001\noptimizer = optim.Adam(net.parameters(), lr = lr)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:00:21.045273Z","iopub.execute_input":"2024-03-26T17:00:21.045749Z","iopub.status.idle":"2024-03-26T17:00:21.465545Z","shell.execute_reply.started":"2024-03-26T17:00:21.045711Z","shell.execute_reply":"2024-03-26T17:00:21.464649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to return the IoU validation score of the best performing network:\ndef highest_IoU(bestnetwork_file_path, device):\n    highest_score = 0\n    if os.path.isfile(checkpoint_file_path):\n        print(\"Best network file found\")\n        if device.type == \"cpu\":\n            file = torch.load(bestnetwork_file_path, torch.device('cpu'))\n            highest_score = file['loss_acc']['Validation accuracy'][-1].item()\n        else:\n            file = torch.load(bestnetwork_file_path, torch.device('cuda'))\n            highest_score = file['loss_acc']['Validation accuracy'][-1].item()\n        print(f\"Using highest accuracy score of {highest_score}\")\n    else:\n        print(\"No previous score found, using default value of 0\")\n    \n    return highest_score\n    \n    \n#If checkpoint file exists, load the file and set model and optimizer parameters\ndef loadprev_training(checkpoint_file_path, device, network, optimizer):\n\n    start_epoch = 0\n    checkpoint_file = None\n    \n    train_loss_array = []\n    train_acc_array = []\n    val_loss_array = []\n    val_acc_array = []\n\n    if os.path.isfile(checkpoint_file_path):\n        print(\"Checkpoint file found\")\n        if device.type == \"cpu\":\n            \n            checkpoint_file = torch.load(checkpoint_file_path, torch.device('cpu'))\n            \n            start_epoch = checkpoint_file['epoch'] + 1\n            \n            network.load_state_dict(checkpoint_file['net_state_dict'])\n            optimizer.load_state_dict(checkpoint_file['optimizer_state_dict'])\n            \n            train_loss_array = (checkpoint_file['loss_acc']['Train loss'])\n            train_acc_array = (checkpoint_file['loss_acc']['Train accuracy'])\n            val_loss_array = (checkpoint_file['loss_acc']['Validation loss'])\n            val_acc_array = (checkpoint_file['loss_acc']['Validation accuracy'])\n            \n        else:\n            checkpoint_file = torch.load(checkpoint_file_path, torch.device('cuda'))\n            \n            start_epoch = checkpoint_file['epoch']+1\n            \n            network.load_state_dict(checkpoint_file['net_state_dict'])\n            optimizer.load_state_dict(checkpoint_file['optimizer_state_dict'])\n            \n            train_loss_array = (checkpoint_file['loss_acc']['Train loss'])\n            train_acc_array = (checkpoint_file['loss_acc']['Train accuracy'])\n            val_loss_array = (checkpoint_file['loss_acc']['Validation loss'])\n            val_acc_array = (checkpoint_file['loss_acc']['Validation accuracy'])\n        print(\"#######################################\")\n        print(f\"Settings loaded succesfully on device: {device}\\nStarting epoch: {start_epoch}\\nCurrent training loss: {train_loss_array[-1]}\\nCurrent IoU Validation score: {val_acc_array[-1]}\")\n        print(\"#######################################\")\n\n    else:\n        print(\"No previous training run file found, using default settings\")\n        \n    return start_epoch, network, optimizer, train_loss_array, train_acc_array, val_loss_array, val_acc_array\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-26T17:00:21.466761Z","iopub.execute_input":"2024-03-26T17:00:21.467248Z","iopub.status.idle":"2024-03-26T17:00:21.478975Z","shell.execute_reply.started":"2024-03-26T17:00:21.46722Z","shell.execute_reply":"2024-03-26T17:00:21.478071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying a summary of the network architecture, note that this can take a while to run due to the size of the network\nsummary(net,(1,1,128,128,128))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:04:50.215639Z","iopub.execute_input":"2024-03-26T17:04:50.21617Z","iopub.status.idle":"2024-03-26T17:05:14.779614Z","shell.execute_reply.started":"2024-03-26T17:04:50.216129Z","shell.execute_reply":"2024-03-26T17:05:14.778538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining the Loss, training and test functions\n\n\n#DiceLoss function that will be used \nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #Sigmoid activiation is part of the network architecture\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice\n    \n#Prediction accuracy measure\ndef IoU(prediction, ground_truth):\n    \n    #We first apply a threshold to the prediction, to ensure a binary mask is made that can be \n    #compared to the ground truth. Ground truth should already be a binary mask\n    \n    threshold = 0.5\n    \n    prediction = (prediction.squeeze()>threshold)*1\n    ground_truth = ground_truth.squeeze()\n    \n\n    pred_diff = prediction.flatten()-ground_truth.flatten()\n    \n    TP = torch.sum((pred_diff == 0)*ground_truth.flatten()) #Amount of true positives\n    FP_or_FN = torch.sum(pred_diff!= 0) #Amount of false positives and false negatives\n    \n    value = TP/(TP+FP_or_FN)\n    \n    return value\n\ndef train(train_loader, net, optimizer, criterion):\n    \"\"\"\n    Trains network for one epoch in batches.\n\n    Args:\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n    \"\"\"\n\n    avg_loss = 0\n    avg_acc = 0\n\n    # iterate through batches\n    for (image,truth) in tqdm.tqdm(train_loader,disable = True):\n        \n        \n        if batch_size == 1:\n            #print('Batch size is 1')\n            image = image.unsqueeze(0)\n            truth = truth.unsqueeze(0)\n        else:\n            #print('Batch size is bigger than 1')\n            #print(image.shape,truth.shape)\n            image = image.unsqueeze(1)\n            truth = truth.unsqueeze(1)\n            #print(image.shape,truth.shape)\n                \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        \n        #Check whether the device we are using is GPU (cuda)\n        #If this is the case, move the training data to the GPU\n        if next(net.parameters()).is_cuda:\n            image = image.to(device)\n            truth = truth.to(device)\n\n        # forward + backward + optimize\n        #print(\"Performing forward step\")\n        output = net(image)\n        \n        #Computing loss\n        #print(\"Computing Loss\")\n        loss = criterion(output, truth)\n        #print(f'Current loss: {loss}')\n        \n        #Computing IoU accuracy\n        #print(\"Computing IoU accuracy\")\n        accuracy = IoU(output, truth)\n        #print(f'Current IoU accuracy: {accuracy}')\n        \n        #print('Performing backward pass')\n        loss.backward()\n        \n        #print('Optimizer step')\n        optimizer.step()\n\n        # keep track of loss and accuracy\n        avg_loss += loss\n        avg_acc += accuracy\n        \n        \n    return avg_loss/len(train_loader), avg_acc/len(train_loader)\n\ndef test(test_loader, net, criterion):\n    \"\"\"\n    Evaluates network in batches.\n\n    Args:\n        test_loader: Data loader for test set.\n        net: Neural network model.\n        criterion: Loss function (e.g. cross-entropy loss).\n    \"\"\"\n\n    avg_loss = 0\n    avg_acc = 0\n\n    # Use torch.no_grad to skip gradient calculation, not needed for evaluation\n    with torch.no_grad():\n        # iterate through batches\n        for (image,truth) in test_loader:\n            \n            if batch_size == 1:\n                image = image.unsqueeze(0)\n                truth = truth.unsqueeze(0)\n            else:\n                image = image.unsqueeze(1)\n                truth = truth.unsqueeze(1)\n            #Check whether the device we are using is GPU (cuda)\n            #If this is the case, move the test data to the GPU\n            if next(net.parameters()).is_cuda:\n                image = image.to(device)\n                truth = truth.to(device)\n            \n            # forward pass\n            output = net(image)\n            loss = criterion(output, truth)\n            accuracy = IoU(output, truth)\n\n            # keep track of loss and accuracy\n            avg_loss += loss\n            avg_acc += accuracy\n            \n    return avg_loss/len(test_loader), avg_acc/len(test_loader)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:07:24.591019Z","iopub.execute_input":"2024-03-26T17:07:24.591482Z","iopub.status.idle":"2024-03-26T17:07:24.612695Z","shell.execute_reply.started":"2024-03-26T17:07:24.591428Z","shell.execute_reply":"2024-03-26T17:07:24.611599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perform training\n\n#Timezone for saving the time of checkpoints made\ntz = pytz.timezone('Europe/Vienna')\n\n# Define the loss function\ncriterion = DiceLoss()\n\nlr = 0.0001\nend_epoch = 150 #Amount of epochs the authors have used of the to be reproduced paper for the U-net\n\n#Set optimizer to Adam\noptimizer = optim.Adam(net.parameters(), lr = lr)\n\n#Defining arrays to store the losses and accuracy scores\ntrain_loss_array = []\ntrain_acc_array = []\nval_loss_array = []\nval_acc_array = []\nhighest_accuracy = 0\n\n#Trying to load previous checkpoint, if it exists:\ncheckpoint_file_path =  \"/kaggle/working/checkpoint_file.tar\"\nbestnetwork_file_path = \"/kaggle/working/bestnetwork.tar\"\nprint(os.path.isfile(checkpoint_file_path), os.path.isfile(bestnetwork_file_path))\n\nstart_epoch, network, optimizer, train_loss_array, train_acc_array, val_loss_array, val_acc_array = loadprev_training(checkpoint_file_path,device,net,optimizer)\nhighest_accuracy = highest_IoU(bestnetwork_file_path, device) #Loading highest accuracy value from best performing network\n\ncheckpoint_saving_location =  os.getcwd() + '/'\nprint(\"##################################\")\nprint(f'Checkpoint saving location: {checkpoint_saving_location}')\n\n#Perform training for #epochs\nfor epoch in range(start_epoch,end_epoch):  # loop over the dataset multiple times\n    \n    print(\"##################################\")\n    print(f'Current epoch: {epoch}')\n    \n    # Train on data\n    train_loss, train_acc = train(train_loader, net, optimizer, criterion)\n    \n    # Test on data\n    val_loss, val_acc = test(val_loader, net, criterion)\n    \n    \n    print(f'Train loss: {train_loss}, Train IoU Accuracy: {train_acc}, Epoch Validation loss: {val_loss}, Epoch Validation IoU Accuracy: {val_acc}')\n    \n    #Appending the scores\n    train_loss_array.append(train_loss.detach())\n    train_acc_array.append(train_acc)\n    val_loss_array.append(val_loss)\n    val_acc_array.append(val_acc)\n    \n    #Every epoch we create a checkpoint where we save the current epoch number,\n    #The state of the model parameters\n    #The optimizer state\n    #The loss and accuracy scores\n    print(\"Checkpoint reached, saving....\")\n    torch.save({\n        'epoch': epoch,\n        'net_state_dict': net.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss_acc': {'Train loss': train_loss_array, 'Train accuracy': train_acc_array,\n                    'Validation loss': val_loss_array, 'Validation accuracy': val_acc_array},\n        'time': datetime.now(tz)},\n        checkpoint_saving_location + 'checkpoint_file.tar')\n    \n    if val_acc>highest_accuracy:\n        print(\"New highest IoU Validation accuracy reached, saving this best network....\")\n        torch.save({\n            'epoch': epoch,\n            'net_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss_acc': {'Train loss': train_loss_array, 'Train accuracy': train_acc_array,\n                        'Validation loss': val_loss_array, 'Validation accuracy': val_acc_array},\n            'time': datetime.now(tz)},\n            checkpoint_saving_location + 'bestnetwork.tar')\n        highest_accuracy = val_acc\n    \n    print(\"##################################\")\n    \nprint('Finished Training')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-26T17:07:29.374438Z","iopub.execute_input":"2024-03-26T17:07:29.37487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}